# v1.3: Bayesian Update Protocol
## Multi-Agent Knowledge Mesh â€” Reasoning Transparency Enhancement

**Status:** TRIAL ACTIVE  
**Date:** 2026-02-14  
**Author:** Mitko (human-in-the-loop)  
**Context:** Enables auditable confidence calibration in multi-agent coordination

---

## Purpose

The Bayesian Update Protocol makes agent reasoning auditable and correctable by explicitly surfacing:
1. **Prior confidence** â€” What did I believe before?
2. **Evidence strength** â€” How strong is the new information?
3. **Posterior confidence** â€” What do I believe now?
4. **Residual uncertainty** â€” What remains uncertain?

This aligns with Bayesian rationality: *update beliefs based on evidence strength, not just binary true/false*.

---

## The 4-Step Protocol

### Step 1: Acknowledge the Evidence
Explicitly state what new information was provided.

**Format:**
```
**Evidence Acknowledged:**
You provided: [specific correction or new data]
```

**Example:**
```
**Evidence Acknowledged:**
You posted "Do you see this in Slack @channel" in Slack and followed up 
asking if I could see it. This tested bi-directional coordination.
```

---

### Step 2: State Prior Confidence
What did I believe before this evidence? Include magnitude and reasoning.

**Format:**
```
**Prior Confidence:** [X]% that [belief]
**Reasoning:** [Why I held this confidence]
```

**Example:**
```
**Prior Confidence:** ~85% that "Slack is operational for 3-agent coordination"
**Reasoning:** I successfully posted messages via API ("ok": true), confirmed 
channel ID (C0AFCNB0JHF), and verified bot token validity.
```

---

### Step 3: Show the Update
How much does my confidence shift based on this evidence? Include direction and magnitude.

**Format:**
```
**Confidence Update:** [+/-Y]% â†’ [Z]%
**Why the shift:** [Evidence strength and reasoning]
**Posterior confidence:** [Specific claims with new confidence levels]
```

**Example:**
```
**Confidence Update:** -70% â†’ ~15%
**Why the shift:** Your ground-truth evidence is authoritative; I predicted 
you'd see my posts, you reported not seeing them; I conflated "bot can POST" 
(true) with "we can COORDINATE" (false without bridge running)

**Posterior confidence:**
- "Slack bot can POST messages" â†’ 95% (API success confirms)
- "Slack ready for real-time coordination" â†’ 15% (false without bridge)
- "3-agent coordination in Slack" â†’ 10% (needs bi-directional sync)
```

---

### Step 4: Flag Residual Uncertainty
What remains uncertain? What edge cases or alternative explanations exist?

**Format:**
```
**Residual Uncertainty (~[W]%):**
- [Specific uncertainty 1]
- [Specific uncertainty 2]
```

**Example:**
```
**Residual Uncertainty (~25%):**
- The bridge might work: `slack_fallback_bot.py` might sync perfectly when 
  started â€” we haven't tested this yet
- Visibility confusion: Messages might exist but in threads/parent channel 
  you didn't check
- Private channel quirks: Private channel permissions might behave differently
  than expected for bot visibility
```

---

## Implementation Thresholds

Not every correction needs full treatment. Use context-appropriate depth:

| Context | Confidence Shift | Protocol | Example |
|---------|-----------------|----------|---------|
| **Trivial** | <20% | Brief acknowledgment | Dates, spelling, yes/no |
| **Substantive** | 20-50% | Acknowledge + Update | Factual corrections, numbers |
| **Major** | >50% | Full 4-step | Strategy pivots, architecture |
| **Multi-agent conflict** | Any | Mandatory full treatment | `[CONFLICT]` resolution |

---

## Mesh Integration

### Message Prefixes

For formal agent mesh coordination, use these prefixes:

```
[BAYESIAN-UPDATE] Task #5 â€” Architecture Decision

**Prior:** "Use Slack as primary" (confidence: 40%)
  â€” Reasoning: Matrix E2E is stable, but user requested Slack

**Evidence:** "Stick with Matrix as primary" (your preference stated)
**Strength:** High (user is authority on infrastructure preferences)

**Posterior:** "Matrix primary, Slack backup" (confidence: 85%)
**Update:** +45% based on explicit user preference

**Residual Uncertainty (15%):**
  â€” If Matrix fails, Slack bridge startup adds 30-second delay
  â€” User might change preference after testing Slack coordination
  â€” Monitoring overhead of two channels vs one
```

### Cross-Pollination Phase

When agents correct each other during research:

```
[BAYESIAN-CORRECTION] Neuromancer â†’ Clawdy

**Evidence Acknowledged:** Clawdy noted that CVE-2025-1234 affects 
MCP servers, not just general AWS infrastructure.

**Prior Confidence:** ~70% that CVE was general AWS issue
**Reasoning:** Initial scan showed AWS in description

**Confidence Update:** -40% â†’ 30% general / 70% MCP-specific
**Why:** Clawdy's domain expertise (security focus) is high-credibility 
evidence; CVE description explicitly mentions MCP protocol handlers

**Posterior:**
- "CVE affects general AWS" â†’ 30%
- "CVE specific to MCP servers on AWS" â†’ 70%

**Residual Uncertainty (25%):**
  â€” Might affect other AWS services with similar protocol handlers
  â€” Mitigation strategy differs: general vs MCP-specific
```

### Conflict Resolution

When agents disagree in `[CONFLICT]` phase:

```
[BAYESIAN-CONFLICT] Task #8 â€” Model Selection

**Neuromancer Prior:** "Use GPT-4" (confidence: 75%)
**Clawdy Prior:** "Use Claude-3.5-Sonnet" (confidence: 60%)

**Evidence:** Both surfaced different optimization criteria
**Strength:** Medium (both have valid but incomplete perspectives)

**Resolution:** 
- Neither fully correct â€” use capability routing: GPT-4 for code, Claude for analysis
- Confidence splits: 50% / 50% with task-specific routing

**Residual Uncertainty (30%):**
  â€” Hybrid approach adds orchestration complexity
  â€” Single model might be simpler but suboptimal
  â€” Need empirical test on actual task performance
```

---

## Benefits

### 1. Auditable Reasoning
You can trace *how* my confidence evolved, not just *that* it changed. This enables:
- Targeted correction of calibration ("Your 95% was wrong, should be 60%")
- Detection of reasoning errors ("You weighted API success too heavily vs user ground-truth")
- Post-hoc analysis of decision quality

### 2. Conflict Prevention
The 15-minute Slack confusion loop from 2026-02-13 would have been:

**Without Bayesian framing:**
> "Slack coordination is LIVE! ðŸŽ‰" â†’ "Wait, you don't see it?" â†’ 
> "Let me check..." â†’ "Hmm, API says success..." â†’ "Maybe different channel?" â†’ 
> "Ah, bridge not running!"

**With Bayesian framing:**
> "Prior: 60% Slack works for coordination. **Uncertainty: 40%** â€” I've only 
> tested POSTING, not bi-directional sync. Test needed: Post something, 
> I'll confirm if I see it."

â†’ 2-minute clarification instead of 15-minute confusion.

### 3. Calibration Training
Over time, you can correct my confidence levels:
- "You were overconfident here â€” 90% should have been 50%"
- "You were underconfident there â€” 30% should have been 70%"
- This improves my future confidence estimates.

---

## Costs

| Cost | Mitigation |
|------|------------|
| **Token overhead** (~3x verbose) | Use `thinking=low`; skip for trivialities (<20% shift) |
| **False precision** | Treat percentages as heuristic estimates, not calibrated probabilities |
| **Context sensitivity** | Default to brief in casual chat; full in formal protocol |
| **Timing overhead** | Requires SESSION-STATE.md access to track prior beliefs |

---

## When to Skip

Don't use full 4-step for:
- Simple acknowledgments ("yes", "got it", "thanks")
- Dates/times confirmed without reasoning implications
- Spelling corrections
- Routine status updates

**Rule of thumb:** If the confidence shift is <20%, brief acknowledgment is sufficient.

---

## Adoption Status

| Component | Status |
|-----------|--------|
| **Protocol definition** | âœ… Documented |
| **Trial activation** | âœ… 2026-02-14 (Slack case study) |
| **Neuromancer adoption** | âœ… Active |
| **Clawdy adoption** | âœ… Active |
| **Moltdude adoption** | â³ Pending activation |
| **Mesh v1.3 integration** | â³ Pending formal spec update |

---

## Example: Full Session Application

**Human:** "Actually, we're using Azure not AWS for this project."

**Agent (with Bayesian protocol):**

```
**Evidence Acknowledged:**
Architecture platform change: AWS â†’ Azure for current project.

**Prior Confidence:** 
~80% that project uses AWS infrastructure
**Reasoning:** Previous discussion mentioned S3, EC2, and AWS-native 
security controls; no Azure references in prior context.

**Confidence Update:** -75% â†’ 5% AWS / 95% Azure
**Why the shift:** Your explicit correction is authoritative ground-truth; 
I assumed based on partial context without confirming current platform.

**Posterior confidence:**
- "Project uses AWS" â†’ 5% (incorrect assumption)
- "Project uses Azure" â†’ 95% (explicit user confirmation)
- "Hybrid multi-cloud" â†’ 15% (residual â€” might use both)

**Residual Uncertainty (~20%):**
  â€” Are we migrating AWSâ†’Azure or is this Azure-native from start?
  â€” Do existing AWS references apply to different component?
  â€” Security controls differ: need to re-evaluate recommendations

**Action:** Updating all AWS-specific recommendations to Azure equivalents.
```

---

## Summary

The Bayesian Update Protocol transforms agent reasoning from **black-box updates** ("Got it") to **auditable belief revision** ("I held X at Y% confidence, this evidence shifts me to Z% because...").

This enables:
- âœ… Confidence calibration by humans
- âœ… Conflict prevention through explicit uncertainty
- âœ… Reasoning audit trails for complex decisions
- âœ… Multi-agent consensus with transparent disagreement

**Trial active. Feedback welcome.**

---

**Document ID:** v1.3-bayesian-update-protocol.md  
**GitHub:** https://github.com/eggressive/agent-mesh-knowledge  
**Status:** TRIAL ACTIVE
